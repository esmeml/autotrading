

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import ElasticNetCV
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline, make_union
from sklearn.preprocessing import FunctionTransformer
from tpot.builtins import StackingEstimator
from tpot.export_utils import set_param_recursive
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
from copy import copy

# Chargement des donn√©es
tpot_data = pd.read_csv("Donnees_CAC40.csv", sep=',')

# Nettoyage
for col in tpot_data.columns:
    if tpot_data[col].dtype == 'object':
        try:
            tpot_data[col] = tpot_data[col].str.replace(",", "")
            tpot_data[col] = tpot_data[col].str.replace(",", ".")
            tpot_data[col] = tpot_data[col].astype(float)
        except:
            pass

tpot_data['Vol.'] = tpot_data['Vol.'].astype(str).str.extract(r'(\d+)')
tpot_data['Vol.'] = tpot_data['Vol.'].astype(float)
tpot_data.drop(columns=['Change %'], inplace=True)

# Rename pour TPOT
tpot_data.rename(columns={"Price": "target"}, inplace=True)
tpot_data.drop(columns=['Date'], inplace=True)

# Split
features = tpot_data.drop('target', axis=1)
target = tpot_data['target']
X_train, X_test, y_train, y_test = train_test_split(features, target, random_state=42, shuffle=False)

# Pipeline export√© par TPOT
exported_pipeline = make_pipeline(
    make_union(
        FunctionTransformer(copy),
        FunctionTransformer(copy)
    ),
    StackingEstimator(estimator=ElasticNetCV(l1_ratio=0.8, tol=0.001)),
    ElasticNetCV(l1_ratio=1.0, tol=1e-05)
)

set_param_recursive(exported_pipeline.steps, 'random_state', 42)

# Entra√Ænement
exported_pipeline.fit(X_train, y_train)
y_pred = exported_pipeline.predict(X_test)

# M√©triques
r2 = r2_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)

print(f"R¬≤ Score du pipeline TPOT : {r2:.4f}")
print(f"Mean Squared Error : {mse:.4f}")
print(f"Mean Absolute Error : {mae:.4f}")

#################################
# VISUALISATIONS
#################################

# Courbe 1 : √âvolution r√©elle vs pr√©dite
plt.figure(figsize=(12, 5))
plt.plot(y_test.values, label="Prix r√©el", color="blue")
plt.plot(y_pred, label="Prix pr√©dit", color="orange")
plt.title("üìà √âvolution du prix r√©el vs pr√©diction")
plt.xlabel("Index (jour)")
plt.ylabel("Prix")
plt.legend()
plt.grid(True)
plt.show()
# üîç Sert √† comparer les dynamiques temporelles entre les vraies valeurs et les pr√©visions

# Courbe 2 : Erreurs de pr√©diction
errors = y_test.values - y_pred
plt.figure(figsize=(12, 5))
plt.plot(errors, label="Erreur de pr√©diction", color="red")
plt.axhline(0, color='black', linestyle='--')
plt.title("üìâ Erreurs de pr√©diction")
plt.xlabel("Index (jour)")
plt.ylabel("Erreur (r√©el - pr√©dit)")
plt.legend()
plt.grid(True)
plt.show()
# üîç Permet d'identifier si les erreurs sont syst√©matiques (biais) ou al√©atoires

# Courbe 3 : Distribution des erreurs
plt.figure(figsize=(8, 5))
plt.hist(errors, bins=30, color="purple", edgecolor="black")
plt.title("üìä Distribution des erreurs")
plt.xlabel("Erreur")
plt.ylabel("Fr√©quence")
plt.grid(True)
plt.show()
# üîç Donne une id√©e de la variance et du biais du mod√®le

# Courbe 4 : Scatter plot R√©el vs Pr√©dit
plt.figure(figsize=(6, 6))
plt.scatter(y_test, y_pred, alpha=0.6, edgecolor='k')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')  # Ligne diagonale parfaite
plt.xlabel("Valeur r√©elle")
plt.ylabel("Valeur pr√©dite")
plt.title("‚öñÔ∏è R√©el vs Pr√©dit")
plt.grid(True)
plt.show()
# üîç Si les points sont proches de la diagonale : mod√®le pr√©cis. Sinon : erreurs importantes



